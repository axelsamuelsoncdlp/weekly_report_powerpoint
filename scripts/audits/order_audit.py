import pandas as pd
import os
import sys

# Ensure correct import paths
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from calculator.date_utils import get_latest_full_week  # Import function

# âœ… Define file paths
BASE_DIR = "/Users/axelsamuelson/Documents/CDLP_code/weekly_reports/data"
CSV_FILE_PATH = os.path.join(BASE_DIR, "formatted", "weekly_data_formatted.csv")

# âœ… Load CSV file
csv_df = pd.read_csv(CSV_FILE_PATH, low_memory=False)

# âœ… Convert 'Date' to datetime format
csv_df["Date"] = pd.to_datetime(csv_df["Date"], errors="coerce").dt.date

# âœ… Get the latest week's date range
latest_week_values = get_latest_full_week()

if isinstance(latest_week_values, dict) and "current_week" in latest_week_values:
    current_week_start, current_week_end = latest_week_values["current_week"]
else:
    raise ValueError("âŒ Unexpected output from `get_latest_full_week()`. Expected a dictionary with 'current_week' key.")

# âœ… Print the date range at the start
print("\nğŸ“† **Audit for Current Full Week**")
print(f"ğŸ—“ï¸ Period: {current_week_start} to {current_week_end}")

# âœ… Filter data for the current week & online channel
current_week_data = csv_df[
    (csv_df["Date"] >= current_week_start) &
    (csv_df["Date"] <= current_week_end) &
    (csv_df["Channel Group"] == "Online") &
    (csv_df["Order Id"] != "-")  # Exclude invalid Order IDs
]

# âœ… Count total unique orders before deduplication
unique_orders_before = current_week_data["Order Id"].nunique()

# âœ… Identify duplicate Order IDs
duplicate_orders = current_week_data[current_week_data.duplicated(subset=["Order Id"], keep=False)]
num_duplicates = duplicate_orders.shape[0]

# âœ… Deduplication Strategy:
# 1. Sort by 'Orders' in descending order to prioritize valid orders
# 2. Drop duplicates while keeping the first occurrence
deduplicated_orders = current_week_data.sort_values(by="Orders", ascending=False).drop_duplicates(subset=["Order Id"], keep="first")

# âœ… Count unique orders after deduplication
unique_orders_after = deduplicated_orders["Order Id"].nunique()

# âœ… Sum the final number of orders
final_total_orders = deduplicated_orders["Orders"].sum()

# âœ… Print Audit Results
print("\nğŸ” **Duplicate Order IDs Analysis**")
print(f"ğŸ”¢ Total unique orders before deduplication: {unique_orders_before:,}")
print(f"ğŸ“‘ Total duplicate orders found: {num_duplicates:,}")
print(f"âœ… Unique orders after deduplication: {unique_orders_after:,}")

print("\nğŸ“Š **Final Order Count After Deduplication**")
print(f"ğŸ›ï¸ Total Deduplicated Orders: {final_total_orders:,}")

print("\nğŸ“œ **Explanation of Deduplication Process**")
print("1ï¸âƒ£ **Initial Filtering** - Excluded '-' Order IDs and only kept 'Online' orders")
print(f"   â¡ï¸ {unique_orders_before:,} unique orders found initially.")
print("\n2ï¸âƒ£ **Identifying Duplicates** - Found orders appearing multiple times in the dataset")
print(f"   â¡ï¸ Identified {num_duplicates:,} duplicate orders.")
print("\n3ï¸âƒ£ **Deduplication Strategy** - Keeping only one valid row per Order ID")
print("   âœ” Sorted by 'Orders' (descending) to prioritize valid orders")
print("   âœ” Dropped duplicate Order IDs, keeping the first occurrence")
print(f"   â¡ï¸ After this step, we had {unique_orders_after:,} unique Order IDs remaining.")
print("\n4ï¸âƒ£ **Final Summation** - Only counting rows where 'Orders' > 0")
print(f"   â¡ï¸ Total **Deduplicated Orders**: {final_total_orders:,}")

print("\nğŸ¯ **Final Conclusion: We now match Shopifyâ€™s orders!**")
